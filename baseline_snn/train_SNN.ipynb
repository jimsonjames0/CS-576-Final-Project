{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcdd0d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jimson/miniconda3/envs/kws-clean/bin/python\n",
      "Torch:  2.4.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import sys, torch\n",
    "print(sys.executable)\n",
    "print(\"Torch: \", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88c5191a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f1ddc1e8b50>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "from pathlib import Path\n",
    "\n",
    "#random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3abf0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[training] total files before keyword filter: 84843\n",
      "[training] labels present (first 15): ['backward', 'bed', 'bird', 'cat', 'dog', 'down', 'eight', 'five', 'follow', 'forward', 'four', 'go', 'happy', 'house', 'learn'] ...\n",
      "[training] files after keyword filter: 18657   keywords=['down', 'go', 'no', 'stop', 'up', 'yes']\n",
      "[validation] total files before keyword filter: 9981\n",
      "[validation] labels present (first 15): ['backward', 'bed', 'bird', 'cat', 'dog', 'down', 'eight', 'five', 'follow', 'forward', 'four', 'go', 'happy', 'house', 'learn'] ...\n",
      "[validation] files after keyword filter: 2252   keywords=['down', 'go', 'no', 'stop', 'up', 'yes']\n",
      "[testing] total files before keyword filter: 11005\n",
      "[testing] labels present (first 15): ['backward', 'bed', 'bird', 'cat', 'dog', 'down', 'eight', 'five', 'follow', 'forward', 'four', 'go', 'happy', 'house', 'learn'] ...\n",
      "[testing] files after keyword filter: 2468   keywords=['down', 'go', 'no', 'stop', 'up', 'yes']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "SAMPLE_RATE = 16000\n",
    "KEYWORDS = [\"yes\", \"no\", \"stop\", \"go\", \"up\", \"down\"]\n",
    "KW_SET = set(KEYWORDS)\n",
    "LABEL2IDX = {k: i for i, k in enumerate(KEYWORDS)}\n",
    "\n",
    "DATA_ROOT = Path(\"data\")\n",
    "DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#converting raw audio data to better representation\n",
    "MFCC = T.MFCC(\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    #features of each audio being retained\n",
    "    n_mfcc=40,\n",
    "    melkwargs={\"n_fft\": 400, \"hop_length\": 160, \"n_mels\": 40, \"center\": False},\n",
    ")\n",
    "\n",
    "#wrapping speechCommands and filtering to key words \n",
    "class KeywordCommands(torchaudio.datasets.SPEECHCOMMANDS):\n",
    "    #calling constructor of parent class \"torchaudio.datasets.SPEECHCOMMANDS\"\n",
    "    def __init__(self, subset: str):\n",
    "        super().__init__(root=str(DATA_ROOT), download=True, subset=subset)\n",
    "        #fid is full file path : /home/jimson/data/SpeechCommands/speech_commands_v0.02/yes/0a7c2a8d_nohash_0.wav\n",
    "        def label_from_fileid(fid: str) -> str:\n",
    "            # Make path relative to dataset root, then take first component as label\n",
    "            # rel = yes/0a7c2a8d_nohash_0.wav\n",
    "            rel = os.path.relpath(fid, self._path)\n",
    "            #splits rel by / or \\ and returns yes\n",
    "            return rel.split(os.sep)[0]\n",
    "\n",
    "        #self._walker is python list of file paths the dataset will use to load samples\n",
    "        all_files = list(self._walker)\n",
    "        #self._walker is list of file paths in the keywords set\n",
    "        self._walker = [f for f in all_files if label_from_fileid(f) in KW_SET]\n",
    "\n",
    "        \n",
    "        print(f\"[{subset}] total files before keyword filter: {len(all_files)}\")\n",
    "        print(f\"[{subset}] files after keyword filter: {len(self._walker)}   keywords={sorted(KW_SET)}\")\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #calling SPEECHCOMMANDS getitem func from loaded audio file at position index in self._walker\n",
    "        #usually comes in 5 tuple (wavefrom, sr, label, speaker_id, utterace num), but *_ ignores rest of values in tuple\n",
    "        waveform, sr, label, *_ = super().__getitem__(index)\n",
    "        # takes [channels, features, Timeframes] and converts to [features, timeFrames];\n",
    "        #what audio sounds like at that timeframe\n",
    "        mfcc = MFCC(waveform).squeeze(0)\n",
    "        #standardizing MFCC feautres so they have zero mean and unnit variance\n",
    "        mfcc = (mfcc - mfcc.mean()) / (mfcc.std() + 1e-6)\n",
    "        #returning mfcc tensor and index to keyword \n",
    "        return mfcc, LABEL2IDX[label]\n",
    "    \n",
    "#mfcc tensors don't all have the same T(time length)\n",
    "#spoken words might have different durations\n",
    "#processing multiple samples together , all tensors need to be of the same shape \n",
    "def pad_collate(batch):\n",
    "    # * is the unpackign operator\n",
    "    tensors, targets = zip(*batch)\n",
    "    #getting mac timeFrame from list\n",
    "    max_len = max(t.shape[1] for t in tensors)\n",
    "    #padding tensors to be the same time_frame\n",
    "    padded = [torch.nn.functional.pad(t, (0, max_len - t.shape[1])) for t in tensors]\n",
    "    #combining all padded [40, max_len] tensors into one big batch tensor and returning: [B, 40, max_len] and [B]\n",
    "    return torch.stack(padded), torch.tensor(targets)\n",
    "\n",
    "\n",
    "train_ds = KeywordCommands(\"training\")\n",
    "val_ds   = KeywordCommands(\"validation\")\n",
    "test_ds  = KeywordCommands(\"testing\")\n",
    "\n",
    "for name, ds in [(\"train\", train_ds), (\"val\", val_ds), (\"test\", test_ds)]:\n",
    "    if len(ds) == 0:\n",
    "        raise RuntimeError(\n",
    "            f\"{name} dataset is empty after filtering.\\n\"\n",
    "        )\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=pad_collate)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, collate_fn=pad_collate)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, collate_fn=pad_collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1257f3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snntorch as snn\n",
    "from snntorch import surrogate\n",
    "\n",
    "class SNNKeywordNet(nn.Module):\n",
    "    def __init__(self, num_classes, beta=0.9, num_steps=8):\n",
    "        super().__init__()\n",
    "        self.num_steps = num_steps\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=5, stride=1, padding=2)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "\n",
    "        spike_grad = surrogate.fast_sigmoid()\n",
    "        self.lif1 = snn.Leaky(beta=beta, spike_grad=spike_grad)\n",
    "        self.lif2 = snn.Leaky(beta= 0.95, spike_grad=spike_grad)\n",
    "        self.lif3 = snn.Leaky(beta= 0.98, spike_grad=spike_grad)\n",
    "\n",
    "        self.fc1 = None\n",
    "        self.fc2 = None\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def _build_heads_if_needed(self, x):\n",
    "        with torch.no_grad():\n",
    "            z1 = self.pool1(self.conv1(x))\n",
    "            z2 = self.pool2(self.conv2(z1))\n",
    "            flat_dim = z2.flatten(start_dim=1).shape[1]\n",
    "        if self.fc1 is None:\n",
    "            self.fc1 = nn.Linear(flat_dim, 64).to(x.device)\n",
    "            self.fc2 = nn.Linear(64, self.num_classes).to(x.device)\n",
    "\n",
    "    def forward(self, x, return_spikes: bool = False):\n",
    "        # x: [B, 40, T] -> [B, 1, 40, T]\n",
    "        x = x.unsqueeze(1)\n",
    "        self._build_heads_if_needed(x)\n",
    "\n",
    "        mem1 = mem2 = mem3 = None\n",
    "        logits_sum = 0.0\n",
    "\n",
    "        # spike counters\n",
    "        if return_spikes:\n",
    "            spikes_l1 = 0.0\n",
    "            spikes_l2 = 0.0\n",
    "            spikes_l3 = 0.0\n",
    "\n",
    "        for _ in range(self.num_steps):\n",
    "            z1 = self.conv1(x)\n",
    "            spk1, mem1 = self.lif1(z1, mem1) if mem1 is not None else self.lif1(z1)\n",
    "            p1 = self.pool1(spk1)\n",
    "\n",
    "            z2 = self.conv2(p1)\n",
    "            spk2, mem2 = self.lif2(z2, mem2) if mem2 is not None else self.lif2(z2)\n",
    "            p2 = self.pool2(spk2)\n",
    "\n",
    "            flat = p2.flatten(start_dim=1)\n",
    "            z3 = self.fc1(flat)\n",
    "            spk3, mem3 = self.lif3(z3, mem3) if mem3 is not None else self.lif3(z3)\n",
    "\n",
    "            logits = self.fc2(spk3)\n",
    "            logits_sum = logits_sum + logits\n",
    "\n",
    "            if return_spikes:\n",
    "                # sum over batch and all units\n",
    "                spikes_l1 += spk1.sum().item()\n",
    "                spikes_l2 += spk2.sum().item()\n",
    "                spikes_l3 += spk3.sum().item()\n",
    "\n",
    "        logits_mean = logits_sum / self.num_steps\n",
    "\n",
    "        if return_spikes:\n",
    "            spike_dict = {\n",
    "                \"layer1\": spikes_l1,\n",
    "                \"layer2\": spikes_l2,\n",
    "                \"layer3\": spikes_l3,\n",
    "                \"total\": spikes_l1 + spikes_l2 + spikes_l3,\n",
    "            }\n",
    "            return logits_mean, spike_dict\n",
    "\n",
    "        return logits_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b045775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check → logits shape: torch.Size([64, 6])\n"
     ]
    }
   ],
   "source": [
    "# Prep: make sure these are imported and the model exists\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# SNNKeywordNet must be defined above (the class you wrote)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = SNNKeywordNet(num_classes=len(KEYWORDS), beta=0.9, num_steps=8).to(device)\n",
    "\n",
    "# (Optional) sanity-check a forward pass using one batch to confirm shapes\n",
    "xb, yb = next(iter(train_loader))\n",
    "with torch.no_grad():\n",
    "    out = model(xb.to(device))\n",
    "print(\"Sanity check → logits shape:\", out.shape)  # should be [batch_size, len(KEYWORDS)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53ddac27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "EPOCHS = 20  # adjust as needed\n",
    "\n",
    "# Functions for one epoch of training and validation\n",
    "def train_epoch(loader):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    for x, y in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * y.size(0)\n",
    "        correct += (out.argmax(1) == y).sum().item()\n",
    "        total += y.size(0)\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch(loader):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    for x, y in tqdm(loader, desc=\"Validating\", leave=False):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        total_loss += loss.item() * y.size(0)\n",
    "        correct += (out.argmax(1) == y).sum().item()\n",
    "        total += y.size(0)\n",
    "    return total_loss / total, correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e399bdb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.793, Test Accuracy: 16.33%\n",
      "Approximate spikes (1 batch): 622354\n",
      "By layer: {'layer1': 523184, 'layer2': 99121, 'layer3': 49}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def count_spikes(model, data_loader, batches=1):\n",
    "    model.eval()\n",
    "    total_spikes = 0.0\n",
    "    by_layer = {\"layer1\": 0.0, \"layer2\": 0.0, \"layer3\": 0.0}\n",
    "    seen = 0\n",
    "    for x, _ in data_loader:\n",
    "        x = x.to(device)\n",
    "        _, spikes = model(x, return_spikes=True)\n",
    "        total_spikes += spikes[\"total\"]\n",
    "        for k in (\"layer1\", \"layer2\", \"layer3\"):\n",
    "            by_layer[k] += spikes[k]\n",
    "        seen += 1\n",
    "        if seen >= batches:\n",
    "            break\n",
    "    return total_spikes, by_layer\n",
    "\n",
    "# Final evaluation on the test set\n",
    "test_loss, test_acc = eval_epoch(test_loader)\n",
    "print(f\"Test Loss: {test_loss:.3f}, Test Accuracy: {test_acc:.2%}\")\n",
    "\n",
    "total_spk, spk_by_layer = count_spikes(model, test_loader, batches=1)\n",
    "print(f\"Approximate spikes (1 batch): {int(total_spk)}\")\n",
    "print(\"By layer:\", {k: int(v) for k, v in spk_by_layer.items()})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kws-clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
