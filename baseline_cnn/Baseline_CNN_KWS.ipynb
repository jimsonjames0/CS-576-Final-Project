{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa28e0da-5445-4efb-9494-5ee2036a4e98",
   "metadata": {},
   "source": [
    "# Baseline_CNN_KWS.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70e4add3-df49-45a5-b29d-aa937f38a9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.0.1\n",
      "Torchaudio: 2.0.2\n",
      "MPS Available: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchaudio.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"Torchaudio:\", torchaudio.__version__)\n",
    "print(\"MPS Available:\", torch.backends.mps.is_available())  # for M1 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55112cdb-d8ab-43a6-a750-317c2135b319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your Speech Commands dataset\n",
    "DATA_DIR = \"../data\"\n",
    "assert os.path.exists(DATA_DIR), \"Dataset folder not found!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23062c5f-2ab6-4c7c-a021-fe09d4cf2ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.DS_Store', 'SpeechCommands']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir(DATA_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "104abf60-bdb5-42b1-9d07-2c71a2f96943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.Rhistory', 'speech_commands_v0.02']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(os.path.join(DATA_DIR, 'SpeechCommands')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a631ee85-2039-4786-ad4a-ac9e0522102c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a small subset for quicker training\n",
    "CLASSES = [\"yes\", \"no\", \"go\", \"stop\", \"down\", \"up\"]\n",
    "\n",
    "# MFCC parameters\n",
    "SAMPLE_RATE = 16000\n",
    "N_MFCC = 40\n",
    "\n",
    "mfcc_transform = T.MFCC(\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    n_mfcc=N_MFCC,\n",
    "    melkwargs={\"n_fft\": 400, \"hop_length\": 160, \"n_mels\": 40, \"center\": False}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7876ff67-a49e-479d-a2c5-378e4590dfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6969f064-bec8-4d92-9e06-af40339aff28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 64721\n",
      "Label: bed\n",
      "Sample rate: 16000\n",
      "Waveform shape: torch.Size([1, 16000])\n"
     ]
    }
   ],
   "source": [
    "from torchaudio.datasets import SPEECHCOMMANDS\n",
    "\n",
    "dataset = SPEECHCOMMANDS(DATA_DIR, download=False)\n",
    "print(f\"Total samples: {len(dataset)}\")\n",
    "\n",
    "waveform, sample_rate, label, *_ = dataset[0]\n",
    "print(\"Label:\", label)\n",
    "print(\"Sample rate:\", sample_rate)\n",
    "print(\"Waveform shape:\", waveform.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53b18b97-333c-4ba9-91f7-2d5fc9a430b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchaudio.datasets import SPEECHCOMMANDS\n",
    "import os\n",
    "\n",
    "class SubsetSC(SPEECHCOMMANDS):\n",
    "    def __init__(self, subset, classes):\n",
    "        super().__init__(root=DATA_DIR, download=False)\n",
    "        self.subset = subset\n",
    "        self.classes = classes\n",
    "        self._walker = self._load_list(subset)\n",
    "\n",
    "    def _load_list(self, subset):\n",
    "        base = os.path.join(self._path)\n",
    "        val_list = os.path.join(base, \"validation_list.txt\")\n",
    "        test_list = os.path.join(base, \"testing_list.txt\")\n",
    "\n",
    "        def read_list(path):\n",
    "            with open(path, \"r\") as f:\n",
    "                return set(line.strip() for line in f)\n",
    "\n",
    "        val_files = read_list(val_list)\n",
    "        test_files = read_list(test_list)\n",
    "\n",
    "        if subset == \"validation\":\n",
    "            return [os.path.join(base, f) for f in val_files if f.split(\"/\")[0] in self.classes]\n",
    "        elif subset == \"testing\":\n",
    "            return [os.path.join(base, f) for f in test_files if f.split(\"/\")[0] in self.classes]\n",
    "        else:\n",
    "            # training = all files not in val or test\n",
    "            all_files = []\n",
    "            for label in self.classes:\n",
    "                folder = os.path.join(base, label)\n",
    "                if os.path.isdir(folder):\n",
    "                    for file in os.listdir(folder):\n",
    "                        path = os.path.join(label, file)\n",
    "                        if path not in val_files and path not in test_files:\n",
    "                            all_files.append(os.path.join(base, path))\n",
    "            return all_files\n",
    "\n",
    "    def __getitem__(self, n):\n",
    "        path = self._walker[n]\n",
    "        waveform, sr = torchaudio.load(path)\n",
    "        label = path.split(\"/\")[-2]\n",
    "        label_idx = self.classes.index(label)\n",
    "        mfcc = mfcc_transform(waveform).squeeze(0)\n",
    "        return mfcc, label_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e62c3f11-4ced-4902-addb-c9311b518a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11144 1561 1533\n"
     ]
    }
   ],
   "source": [
    "train_set = SubsetSC(\"training\", CLASSES)\n",
    "val_set   = SubsetSC(\"validation\", CLASSES)\n",
    "test_set  = SubsetSC(\"testing\", CLASSES)\n",
    "\n",
    "print(len(train_set), len(val_set), len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "416de6de-1926-4cf2-8ca4-55b6f5adf49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CNN_KWS(nn.Module):\n",
    "    def __init__(self, num_classes=len(CLASSES)):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16 * 10 * 20, 64),   # adjust based on MFCC shape\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch, 40, time]\n",
    "        x = x.unsqueeze(1)  # add channel dim â†’ [batch, 1, 40, time]\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e030cc9-5d8c-4451-8064-41ddea619ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_KWS(\n",
      "  (net): Sequential(\n",
      "    (0): Conv2d(1, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Flatten(start_dim=1, end_dim=-1)\n",
      "    (7): Linear(in_features=3200, out_features=64, bias=True)\n",
      "    (8): ReLU()\n",
      "    (9): Linear(in_features=64, out_features=6, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "model = CNN_KWS().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e6ffa9c-2f4e-4008-bf06-c723f61d622e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m criterion \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m      2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n\u001b[1;32m      3\u001b[0m EPOCHS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b96384e2-f869-4ccb-996d-27d7836a3a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_epoch(loader):\n",
    "    model.train()\n",
    "    total_loss, correct = 0, 0\n",
    "    for x, y in tqdm(loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        correct += (out.argmax(1) == y).sum().item()\n",
    "    return total_loss / len(loader), correct / len(loader.dataset)\n",
    "\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    total_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            out = model(x)\n",
    "            total_loss += criterion(out, y).item()\n",
    "            correct += (out.argmax(1) == y).sum().item()\n",
    "    return total_loss / len(loader), correct / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f9c88dc-e3a3-4275-ad6d-210b7c9ba715",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def pad_sequence(batch):\n",
    "    # batch: list of tuples (mfcc, label)\n",
    "    tensors, targets = zip(*batch)\n",
    "\n",
    "    # Find max time dimension in batch\n",
    "    max_len = max(t.shape[1] for t in tensors)\n",
    "\n",
    "    # Pad each MFCC to max_len\n",
    "    padded = []\n",
    "    for t in tensors:\n",
    "        pad_amount = max_len - t.shape[1]\n",
    "        padded_t = F.pad(t, (0, pad_amount))  # pad on time axis\n",
    "        padded.append(padded_t)\n",
    "\n",
    "    padded = torch.stack(padded)  # [batch, 40, time]\n",
    "    targets = torch.tensor(targets)\n",
    "    return padded, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad3539cd-b410-416d-a322-5a28b2bca0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 175 | Val batches: 25 | Test batches: 24\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, collate_fn=pad_sequence)\n",
    "val_loader   = DataLoader(val_set, batch_size=BATCH_SIZE, collate_fn=pad_sequence)\n",
    "test_loader  = DataLoader(test_set, batch_size=BATCH_SIZE, collate_fn=pad_sequence)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)} | Val batches: {len(val_loader)} | Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7a8677d-4729-46c4-b9bc-e6658e87dbe2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'EPOCHS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m train_acc, val_acc \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[43mEPOCHS\u001b[49m):\n\u001b[1;32m      4\u001b[0m     tr_loss, tr_acc \u001b[38;5;241m=\u001b[39m train_epoch(train_loader)\n\u001b[1;32m      5\u001b[0m     v_loss, v_acc \u001b[38;5;241m=\u001b[39m evaluate(val_loader)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'EPOCHS' is not defined"
     ]
    }
   ],
   "source": [
    "train_acc, val_acc = [], []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    tr_loss, tr_acc = train_epoch(train_loader)\n",
    "    v_loss, v_acc = evaluate(val_loader)\n",
    "    train_acc.append(tr_acc)\n",
    "    val_acc.append(v_acc)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}: Train Acc={tr_acc:.3f}, Val Acc={v_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c8dd19-63a0-475d-878d-7505c305e077",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9eaea62-cd94-4ec5-8101-cd48e4b2e14c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40eb37c-54df-430e-a343-47523ec4027c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
