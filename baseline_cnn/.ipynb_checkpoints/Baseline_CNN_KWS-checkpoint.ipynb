{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa28e0da-5445-4efb-9494-5ee2036a4e98",
   "metadata": {
    "id": "aa28e0da-5445-4efb-9494-5ee2036a4e98"
   },
   "source": [
    "# Baseline_CNN_KWS.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2JxEyeyp9zE0",
   "metadata": {
    "id": "2JxEyeyp9zE0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: /Users/maddy/Desktop/PLEP/Project/CS-576-Final-Project\n",
      "DATA_ROOT exists: True\n",
      "MODEL_DIR exists: True\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Baseline CNN for 6-class Keyword Spotting\n",
    "# Local training on sample_data/speech_commands_v0.02\n",
    "# Compatible with SNN_Conversion + Loihi notebooks.\n",
    "\n",
    "# %%\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import soundfile as sf\n",
    "from tqdm import tqdm\n",
    "\n",
    "# If you run this notebook from baseline_cnn/, project root is its parent\n",
    "PROJECT_ROOT = Path.cwd().resolve().parent\n",
    "DATA_ROOT = PROJECT_ROOT / \"sample_data\" / \"speech_commands_v0.02\"\n",
    "MODEL_DIR = PROJECT_ROOT / \"saved_models\"\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"DATA_ROOT exists:\", DATA_ROOT.exists())\n",
    "print(\"MODEL_DIR exists:\", MODEL_DIR.exists())\n",
    "\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee50bf5e-c694-4966-ac7e-e71ab7341925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Target classes (same as SNN + Loihi)\n",
    "CLASSES = [\"yes\", \"no\", \"go\", \"stop\", \"down\", \"up\"]\n",
    "\n",
    "SAMPLE_RATE = 16000\n",
    "N_MFCC = 40\n",
    "\n",
    "# MFCC pipeline (no torchcodec)\n",
    "mfcc_transform = T.MFCC(\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    n_mfcc=N_MFCC,\n",
    "    melkwargs={\n",
    "        \"n_fft\": 400,\n",
    "        \"hop_length\": 160,\n",
    "        \"n_mels\": 40,\n",
    "        \"center\": False,\n",
    "    },\n",
    ")\n",
    "\n",
    "def wav_to_mfcc(path: Path) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Load WAV using soundfile, resample with torch, then compute MFCC.\n",
    "    Output: [40, T]\n",
    "    \"\"\"\n",
    "    # soundfile returns numpy array\n",
    "    waveform_np, sr = sf.read(str(path))\n",
    "    waveform = torch.tensor(waveform_np, dtype=torch.float32)\n",
    "\n",
    "    # Ensure shape [1, num_samples]\n",
    "    if waveform.ndim == 1:\n",
    "        waveform = waveform.unsqueeze(0)\n",
    "    elif waveform.ndim == 2:\n",
    "        # If stereo, convert to mono\n",
    "        waveform = waveform.mean(dim=1, keepdim=True).transpose(0, 1)\n",
    "\n",
    "    if sr != SAMPLE_RATE:\n",
    "        waveform = torchaudio.functional.resample(waveform, sr, SAMPLE_RATE)\n",
    "\n",
    "    mfcc = mfcc_transform(waveform).squeeze(0)  # [40, T]\n",
    "\n",
    "    # per-sample normalization (matches SNN notebook style)\n",
    "    mfcc = (mfcc - mfcc.mean()) / (mfcc.std() + 1e-6)\n",
    "\n",
    "    # clamp for stability\n",
    "    mfcc = torch.clamp(mfcc, -2.0, 2.0)\n",
    "    return mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a391d6ff-4d84-4ce6-b227-25ee55f6b79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num val entries in file: 2252\n",
      "Num test entries in file: 2468\n",
      "Train files: 18657\n",
      "Val files:   2252\n",
      "Test files:  2468\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# We will replicate the Speech Commands official split:\n",
    "#   - validation_list.txt\n",
    "#   - testing_list.txt\n",
    "#   - train = remaining files\n",
    "\n",
    "val_list_path = DATA_ROOT / \"validation_list.txt\"\n",
    "test_list_path = DATA_ROOT / \"testing_list.txt\"\n",
    "\n",
    "def read_split_list(list_path: Path):\n",
    "    \"\"\"Returns a set of relative paths like 'yes/xxxx.wav'.\"\"\"\n",
    "    lines = []\n",
    "    with open(list_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            rel = line.strip()\n",
    "            # Only keep our target classes\n",
    "            if rel and rel.split(\"/\")[0] in CLASSES:\n",
    "                lines.append(rel)\n",
    "    return set(lines)\n",
    "\n",
    "val_rel = read_split_list(val_list_path)\n",
    "test_rel = read_split_list(test_list_path)\n",
    "\n",
    "print(\"Num val entries in file:\", len(val_rel))\n",
    "print(\"Num test entries in file:\", len(test_rel))\n",
    "\n",
    "# Collect full Paths for each split\n",
    "train_files = []\n",
    "val_files = []\n",
    "test_files = []\n",
    "\n",
    "# Use the actual folder structure under DATA_ROOT\n",
    "for cls in CLASSES:\n",
    "    cls_dir = DATA_ROOT / cls\n",
    "    if not cls_dir.exists():\n",
    "        print(f\"Missing class folder: {cls_dir}\")\n",
    "        continue\n",
    "\n",
    "    for wav_path in cls_dir.glob(\"*.wav\"):\n",
    "        rel = f\"{cls}/{wav_path.name}\"\n",
    "        if rel in val_rel:\n",
    "            val_files.append(wav_path)\n",
    "        elif rel in test_rel:\n",
    "            test_files.append(wav_path)\n",
    "        else:\n",
    "            train_files.append(wav_path)\n",
    "\n",
    "print(f\"Train files: {len(train_files)}\")\n",
    "print(f\"Val files:   {len(val_files)}\")\n",
    "print(f\"Test files:  {len(test_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4279f5a-5f22-4402-8393-3e3fb79928cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "class KWS_Dataset(Dataset):\n",
    "    def __init__(self, file_list, classes):\n",
    "        self.files = list(file_list)\n",
    "        self.classes = classes\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.files[idx]\n",
    "        mfcc = wav_to_mfcc(path)  # [40, T]\n",
    "        label = path.parent.name\n",
    "        y = self.classes.index(label)\n",
    "        return mfcc, y\n",
    "\n",
    "\n",
    "def pad_collate(batch):\n",
    "    \"\"\"\n",
    "    Batch: list of (mfcc [40,T_i], y)\n",
    "    We pad along time dimension to max T.\n",
    "    \"\"\"\n",
    "    xs, ys = zip(*batch)\n",
    "    max_t = max(x.shape[1] for x in xs)\n",
    "    xs_pad = [F.pad(x, (0, max_t - x.shape[1])) for x in xs]  # pad time dim\n",
    "    xs_pad = torch.stack(xs_pad)  # [B, 40, T_max]\n",
    "    ys = torch.tensor(ys, dtype=torch.long)\n",
    "    return xs_pad, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bce9dc6b-c599-45ea-8875-4333943f6e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 292\n",
      "Val batches:   36\n",
      "Test batches:  39\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataset = KWS_Dataset(train_files, CLASSES)\n",
    "val_dataset   = KWS_Dataset(val_files, CLASSES)\n",
    "test_dataset  = KWS_Dataset(test_files, CLASSES)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=pad_collate,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=pad_collate,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=pad_collate,\n",
    ")\n",
    "\n",
    "print(\"Train batches:\", len(train_loader))\n",
    "print(\"Val batches:  \", len(val_loader))\n",
    "print(\"Test batches: \", len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36523274-2784-486c-aba5-01f0099dc8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_KWS(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(1, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): LazyLinear(in_features=0, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=6, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "class CNN_KWS(nn.Module):\n",
    "    def __init__(self, num_classes=6):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        # LazyLinear will infer flatten_dim from first forward pass\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LazyLinear(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, 40, T]\n",
    "        x = x.unsqueeze(1)  # [B,1,40,T]\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)  # [B, F]\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate model\n",
    "model = CNN_KWS(num_classes=len(CLASSES)).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "316a9961-0aef-4f02-82b9-d57a0ee5e2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for mfcc_batch, y_batch in tqdm(loader, desc=\"Train\", leave=False):\n",
    "        mfcc_batch = mfcc_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(mfcc_batch)\n",
    "        loss = criterion(out, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * y_batch.size(0)\n",
    "        preds = out.argmax(dim=1)\n",
    "        correct += (preds == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "\n",
    "    avg_loss = running_loss / max(total, 1)\n",
    "    acc = correct / max(total, 1)\n",
    "    return avg_loss, acc\n",
    "\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for mfcc_batch, y_batch in tqdm(loader, desc=\"Eval\", leave=False):\n",
    "            mfcc_batch = mfcc_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            out = model(mfcc_batch)\n",
    "            loss = criterion(out, y_batch)\n",
    "\n",
    "            running_loss += loss.item() * y_batch.size(0)\n",
    "            preds = out.argmax(dim=1)\n",
    "            correct += (preds == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "\n",
    "    avg_loss = running_loss / max(total, 1)\n",
    "    acc = correct / max(total, 1)\n",
    "    return avg_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbd53f56-70b0-44e1-abad-440948ad9192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Epoch 1/10 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss=1.1990, acc=52.74% | Val: loss=0.9640, acc=63.63%\n",
      "✅ Saved new best model (val acc = 63.63%)\n",
      "\n",
      "=== Epoch 2/10 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss=0.8448, acc=68.64% | Val: loss=0.8051, acc=69.45%\n",
      "✅ Saved new best model (val acc = 69.45%)\n",
      "\n",
      "=== Epoch 3/10 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss=0.7070, acc=73.96% | Val: loss=0.6788, acc=75.71%\n",
      "✅ Saved new best model (val acc = 75.71%)\n",
      "\n",
      "=== Epoch 4/10 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss=0.6043, acc=77.82% | Val: loss=0.6452, acc=76.78%\n",
      "✅ Saved new best model (val acc = 76.78%)\n",
      "\n",
      "=== Epoch 5/10 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss=0.5301, acc=80.69% | Val: loss=0.6137, acc=78.51%\n",
      "✅ Saved new best model (val acc = 78.51%)\n",
      "\n",
      "=== Epoch 6/10 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss=0.4385, acc=84.46% | Val: loss=0.5875, acc=79.80%\n",
      "✅ Saved new best model (val acc = 79.80%)\n",
      "\n",
      "=== Epoch 7/10 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss=0.4033, acc=85.74% | Val: loss=0.5830, acc=80.55%\n",
      "✅ Saved new best model (val acc = 80.55%)\n",
      "\n",
      "=== Epoch 8/10 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss=0.3758, acc=86.56% | Val: loss=0.5866, acc=80.64%\n",
      "✅ Saved new best model (val acc = 80.64%)\n",
      "\n",
      "=== Epoch 9/10 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss=0.3448, acc=87.99% | Val: loss=0.5790, acc=81.22%\n",
      "✅ Saved new best model (val acc = 81.22%)\n",
      "\n",
      "=== Epoch 10/10 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss=0.3189, acc=88.69% | Val: loss=0.5861, acc=81.79%\n",
      "✅ Saved new best model (val acc = 81.79%)\n",
      "\n",
      "Training finished.\n",
      "Best validation accuracy: 81.79%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# %%\n",
    "EPOCHS = 10\n",
    "best_val_acc = 0.0\n",
    "best_state = None\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(f\"\\n=== Epoch {epoch}/{EPOCHS} ===\")\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, device)\n",
    "    val_loss, val_acc = evaluate(model, val_loader, device)\n",
    "    scheduler.step()\n",
    "\n",
    "    print(\n",
    "        f\"Train: loss={train_loss:.4f}, acc={train_acc*100:.2f}% | \"\n",
    "        f\"Val: loss={val_loss:.4f}, acc={val_acc*100:.2f}%\"\n",
    "    )\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_state = model.state_dict()\n",
    "        torch.save(best_state, MODEL_DIR / \"baseline_cnn_kws_vfinal.pt\")\n",
    "        print(f\"Saved new best model (val acc = {best_val_acc*100:.2f}%)\")\n",
    "\n",
    "print(\"\\nTraining finished.\")\n",
    "print(f\"Best validation accuracy: {best_val_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e509699b-230a-4ce8-a470-cb928a877afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Final Test Accuracy: 80.43%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# %%\n",
    "best_path = MODEL_DIR / \"baseline_cnn_kws_vfinal.pt\"\n",
    "assert best_path.exists(), f\"Checkpoint not found: {best_path}\"\n",
    "\n",
    "best_model = CNN_KWS(num_classes=len(CLASSES)).to(device)\n",
    "# trigger LazyLinear init\n",
    "with torch.no_grad():\n",
    "    # use one batch to infer flatten_dim\n",
    "    for mfcc_batch, _ in train_loader:\n",
    "        mfcc_batch = mfcc_batch.to(device)\n",
    "        _ = best_model(mfcc_batch)\n",
    "        break\n",
    "\n",
    "best_state = torch.load(best_path, map_location=device)\n",
    "best_model.load_state_dict(best_state)\n",
    "best_model.eval()\n",
    "\n",
    "test_loss, test_acc = evaluate(best_model, test_loader, device)\n",
    "print(f\"Final Test Accuracy: {test_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fa5a1af-969f-4b24-8bcb-03dc97e2475a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN model saved to: /Users/maddy/Desktop/PLEP/Project/CS-576-Final-Project/saved_models/baseline_cnn_kws_vfinal.pt\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Save final CNN model\n",
    "save_path = MODEL_DIR / \"baseline_cnn_kws_vfinal.pt\"\n",
    "torch.save(model.state_dict(), save_path)\n",
    "\n",
    "print(f\"CNN model saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3db12d5-84e4-42b8-88ad-175361a4eb78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
