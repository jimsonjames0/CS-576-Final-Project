{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "91b48e67-aac0-4c04-8e7e-32b2829aa7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: /Users/maddy/Desktop/PLEP/Project/CS-576-Final-Project\n",
      "DATA_ROOT exists: True\n",
      "MODEL_DIR exists: True\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import soundfile as sf\n",
    "\n",
    "import nengo\n",
    "import nengo_loihi\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Paths\n",
    "# -------------------------------------------------\n",
    "PROJECT_ROOT = Path.cwd().resolve().parent   # parent of loihi_emulator\n",
    "DATA_ROOT = PROJECT_ROOT / \"sample_data\" / \"speech_commands_v0.02\"\n",
    "MODEL_DIR = PROJECT_ROOT / \"saved_models\"\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"DATA_ROOT exists:\", DATA_ROOT.exists())\n",
    "print(\"MODEL_DIR exists:\", MODEL_DIR.exists())\n",
    "\n",
    "device = torch.device(\"cpu\")  # Loihi sim is CPU-only anyway\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Restrict to the same 6 classes\n",
    "CLASSES = [\"yes\", \"no\", \"go\", \"stop\", \"down\", \"up\"]\n",
    "NUM_CLASSES = len(CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a871d1b1-f03f-4a99-aa49-5d00d8df339a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "class CNN_KWS(nn.Module):\n",
    "    def __init__(self, num_classes=6, flatten_dim=3840):\n",
    "        super().__init__()\n",
    "\n",
    "        self.flatten_dim = flatten_dim  # in_features of first FC layer\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),          # 40xT -> 20xT/2\n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),          # 20xT/2 -> 10xT/4\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.flatten_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, 40, T]\n",
    "        x = x.unsqueeze(1)           # [B,1,40,T]\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)      # [B,F]\n",
    "\n",
    "        # Safety: crop / pad to expected flatten_dim\n",
    "        F_now = x.shape[1]\n",
    "        if F_now > self.flatten_dim:\n",
    "            x = x[:, :self.flatten_dim]\n",
    "        elif F_now < self.flatten_dim:\n",
    "            pad = self.flatten_dim - F_now\n",
    "            x = F.pad(x, (0, pad))\n",
    "\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "47459624-efee-46af-af6f-c64f5a1cc2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN checkpoint exists: True\n",
      "Flatten dim in checkpoint: 3840\n",
      "CNN model loaded successfully.\n",
      "fc2 weight shape: (6, 64)\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "cnn_ckpt_path = MODEL_DIR / \"baseline_cnn_kws_vfinal.pt\"\n",
    "print(\"CNN checkpoint exists:\", cnn_ckpt_path.exists())\n",
    "\n",
    "# Peek into state_dict to get correct flatten_dim\n",
    "state_dict = torch.load(cnn_ckpt_path, map_location=device)\n",
    "flatten_dim_ckpt = state_dict[\"classifier.0.weight\"].shape[1]\n",
    "print(\"Flatten dim in checkpoint:\", flatten_dim_ckpt)\n",
    "\n",
    "cnn_model = CNN_KWS(num_classes=NUM_CLASSES, flatten_dim=flatten_dim_ckpt).to(device)\n",
    "cnn_model.load_state_dict(state_dict)\n",
    "cnn_model.eval()\n",
    "print(\"CNN model loaded successfully.\")\n",
    "\n",
    "# Grab final FC weights for Loihi classifier\n",
    "fc2 = cnn_model.classifier[2]\n",
    "W = fc2.weight.detach().cpu().numpy()  # [6, 64]\n",
    "b = fc2.bias.detach().cpu().numpy()    # [6]\n",
    "print(\"fc2 weight shape:\", W.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "701e3a74-fbf0-48f5-9c2b-69a573205094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "SAMPLE_RATE = 16000\n",
    "N_MFCC = 40\n",
    "\n",
    "mfcc_transform = nn.Sequential(\n",
    "    T.MFCC(\n",
    "        sample_rate=SAMPLE_RATE,\n",
    "        n_mfcc=N_MFCC,\n",
    "        melkwargs={\n",
    "            \"n_fft\": 400,\n",
    "            \"hop_length\": 160,\n",
    "            \"n_mels\": 40,\n",
    "            \"center\": False,\n",
    "        },\n",
    "    ),\n",
    "    T.AmplitudeToDB(),\n",
    ")\n",
    "\n",
    "def wav_to_mfcc(path: Path) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Read WAV with soundfile (avoids TorchCodec), resample if needed,\n",
    "    then compute normalized MFCC [40, T].\n",
    "    \"\"\"\n",
    "    waveform, sr = sf.read(str(path))\n",
    "    waveform = torch.tensor(waveform).float()\n",
    "\n",
    "    # Ensure shape [1, N]\n",
    "    if waveform.ndim == 1:\n",
    "        waveform = waveform.unsqueeze(0)\n",
    "    elif waveform.ndim == 2 and waveform.shape[1] > waveform.shape[0]:\n",
    "        waveform = waveform.T  # [C,N]\n",
    "\n",
    "    if sr != SAMPLE_RATE:\n",
    "        waveform = torchaudio.functional.resample(waveform, sr, SAMPLE_RATE)\n",
    "\n",
    "    mfcc = mfcc_transform(waveform).squeeze(0)  # [40,T]\n",
    "    mfcc = (mfcc - mfcc.mean()) / (mfcc.std() + 1e-6)\n",
    "    mfcc = torch.clamp(mfcc, -2.0, 2.0)\n",
    "    return mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1f388f26-60f0-4e93-be15-c49e15ae2bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total WAV files in 6 classes: 23377\n",
      "Loaded test samples: 23377\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "class KWS_Dataset(Dataset):\n",
    "    def __init__(self, files, classes):\n",
    "        self.files = files\n",
    "        self.classes = classes\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.files[idx]\n",
    "        mfcc = wav_to_mfcc(path)    # [40,T]\n",
    "        label = path.parent.name\n",
    "        y = self.classes.index(label)\n",
    "        return mfcc, y\n",
    "\n",
    "\n",
    "def pad_collate(batch):\n",
    "    xs, ys = zip(*batch)\n",
    "    max_t = max(x.shape[1] for x in xs)\n",
    "    xs = [F.pad(x, (0, max_t - x.shape[1])) for x in xs]\n",
    "    xs = torch.stack(xs)   # [B,40,T]\n",
    "    ys = torch.tensor(ys)\n",
    "    return xs, ys\n",
    "\n",
    "\n",
    "# Build a test file list\n",
    "file_list = []\n",
    "for c in CLASSES:\n",
    "    class_dir = DATA_ROOT / c\n",
    "    file_list += sorted(class_dir.glob(\"*.wav\"))\n",
    "\n",
    "print(\"Total WAV files in 6 classes:\", len(file_list))\n",
    "\n",
    "# Shuffle and optionally subsample\n",
    "random.seed(0)\n",
    "random.shuffle(file_list)\n",
    "\n",
    "test_files = file_list  # you can slice e.g. file_list[:1000]\n",
    "\n",
    "test_dataset = KWS_Dataset(test_files, CLASSES)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    collate_fn=pad_collate,\n",
    ")\n",
    "\n",
    "print(\"Loaded test samples:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8c24dda7-d6b7-47d1-9e99-86b5d20d37f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example file: /Users/maddy/Desktop/PLEP/Project/CS-576-Final-Project/sample_data/speech_commands_v0.02/stop/b84f83d2_nohash_0.wav\n",
      "Feat shape: torch.Size([1, 64])\n",
      "CNN predicts: yes\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "def extract_cnn_features(x: torch.Tensor, model: CNN_KWS) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    x: [B,40,T]\n",
    "    returns: [B,64] feature vector after first FC + ReLU\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        x_ = x.unsqueeze(1)     # [B,1,40,T]\n",
    "        h = model.features(x_)\n",
    "        h = torch.flatten(h, 1)\n",
    "        # crop/pad like in forward\n",
    "        F_now = h.shape[1]\n",
    "        if F_now > model.flatten_dim:\n",
    "            h = h[:, :model.flatten_dim]\n",
    "        elif F_now < model.flatten_dim:\n",
    "            pad = model.flatten_dim - F_now\n",
    "            h = F.pad(h, (0, pad))\n",
    "        fc1 = model.classifier[0]\n",
    "        h = F.relu(fc1(h))\n",
    "    return h   # [B,64]\n",
    "\n",
    "\n",
    "# Quick sanity check on one example\n",
    "from pathlib import Path as _P\n",
    "\n",
    "example_path = next(iter(test_files))\n",
    "print(\"Example file:\", example_path)\n",
    "\n",
    "mfcc_ex = wav_to_mfcc(example_path).unsqueeze(0).to(device)  # [1,40,T]\n",
    "with torch.no_grad():\n",
    "    feats_ex = extract_cnn_features(mfcc_ex, cnn_model)\n",
    "    logits_ex = cnn_model(mfcc_ex)\n",
    "    pred_idx = logits_ex.argmax(dim=1).item()\n",
    "\n",
    "print(\"Feat shape:\", feats_ex.shape)\n",
    "print(\"CNN predicts:\", CLASSES[pred_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "324a4654-4413-4da5-93fe-e46236f22d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def run_loihi_for_feature(\n",
    "    feat_vec: np.ndarray,\n",
    "    W: np.ndarray,\n",
    "    sim_time: float = 0.1\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Run a 64-D feature vector through a tiny LIF ensemble on the\n",
    "    Loihi emulator. W is [6,64] (fc2 weights).\n",
    "    Returns logits_loihi [6].\n",
    "    \"\"\"\n",
    "    assert feat_vec.shape == (64,), f\"Expected (64,), got {feat_vec.shape}\"\n",
    "    assert W.shape == (NUM_CLASSES, 64), f\"W shape {W.shape} unexpected\"\n",
    "\n",
    "    with nengo.Network(seed=0) as net:\n",
    "        inp = nengo.Node(output=lambda t: feat_vec)\n",
    "\n",
    "        ens = nengo.Ensemble(\n",
    "            n_neurons=64,\n",
    "            dimensions=64,\n",
    "            neuron_type=nengo.LIF(),\n",
    "        )\n",
    "\n",
    "        out = nengo.Node(size_in=NUM_CLASSES)\n",
    "\n",
    "        nengo.Connection(inp, ens, synapse=None)\n",
    "        nengo.Connection(\n",
    "            ens.neurons,\n",
    "            out,\n",
    "            transform=W,\n",
    "            synapse=0.01,\n",
    "        )\n",
    "\n",
    "        p_out = nengo.Probe(out, synapse=0.01)\n",
    "\n",
    "    with nengo_loihi.Simulator(net) as sim:\n",
    "        sim.run(sim_time)\n",
    "        logits_loihi = sim.data[p_out][-1]\n",
    "\n",
    "    logits_loihi = np.nan_to_num(logits_loihi)\n",
    "    return logits_loihi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "28f76bc9-2520-4a29-b489-af8b1b89714e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from typing import Tuple\n",
    "\n",
    "def eval_loihi_classifier(\n",
    "    loader,\n",
    "    cnn_model: CNN_KWS,\n",
    "    W: np.ndarray,\n",
    "    device: torch.device,\n",
    "    max_samples: int = 100,\n",
    "    sim_time: float = 0.1,\n",
    ") -> Tuple[float, float, int]:\n",
    "\n",
    "    cnn_model.eval()\n",
    "    total = 0\n",
    "    correct_cnn = 0\n",
    "    correct_loihi = 0\n",
    "\n",
    "    for mfcc_batch, y_batch in loader:\n",
    "        mfcc_batch = mfcc_batch.to(device)\n",
    "        y_batch_np = y_batch.numpy()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            feats = extract_cnn_features(mfcc_batch, cnn_model)   # [B,64]\n",
    "            fc2 = cnn_model.classifier[2]\n",
    "            logits_cnn = fc2(feats)                               # [B,6]\n",
    "            preds_cnn = logits_cnn.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "        batch_size = feats.size(0)\n",
    "        for i in range(batch_size):\n",
    "            feat_np = feats[i].cpu().numpy()\n",
    "            label = int(y_batch_np[i])\n",
    "\n",
    "            # CNN prediction\n",
    "            if preds_cnn[i] == label:\n",
    "                correct_cnn += 1\n",
    "\n",
    "            # Loihi prediction\n",
    "            logits_loihi = run_loihi_for_feature(feat_np, W=W, sim_time=sim_time)\n",
    "            pred_loihi = int(np.argmax(logits_loihi))\n",
    "            if pred_loihi == label:\n",
    "                correct_loihi += 1\n",
    "\n",
    "            total += 1\n",
    "            if total >= max_samples:\n",
    "                cnn_acc = correct_cnn / total\n",
    "                loihi_acc = correct_loihi / total\n",
    "                return cnn_acc, loihi_acc, total\n",
    "\n",
    "    cnn_acc = correct_cnn / max(total, 1)\n",
    "    loihi_acc = correct_loihi / max(total, 1)\n",
    "    return cnn_acc, loihi_acc, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e4fc8427-fc22-47f3-9e3b-71781de55314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated on 50 test samples\n",
      "CNN head accuracy:    44.00%\n",
      "Loihi classifier acc: 20.00%\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "max_samples = 50    # or 100 if you want\n",
    "sim_time = 0.1      # 100 ms per sample\n",
    "\n",
    "cnn_acc, loihi_acc, total = eval_loihi_classifier(\n",
    "    loader=test_loader,\n",
    "    cnn_model=cnn_model,\n",
    "    W=W,\n",
    "    device=device,\n",
    "    max_samples=max_samples,\n",
    "    sim_time=sim_time,\n",
    ")\n",
    "\n",
    "print(f\"Evaluated on {total} test samples\")\n",
    "print(f\"CNN head accuracy:    {cnn_acc*100:.2f}%\")\n",
    "print(f\"Loihi classifier acc: {loihi_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed1ac15-d555-48da-b9b2-daba2cf856af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
